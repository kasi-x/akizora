著作権切れ文章データの多言語翻訳(グーテンブルグを中心に)

# 成果

スクレイピングとデータ構造に関する解像度の高い知見を得た
開始前よりもゴールから遠ざかった。問題のめんどくささを痛感した。
翻訳結果については、まだ詳細な精度確認のテストが行っておらず、公開できる状態にない。
期間中システム構築を進めた。

# 前提

前提として、参加前に少量のデータをスクレイピングしてLLMにかけて翻訳するトライアルのスクリプトと公開用のCMSを用意していた。
それを踏まえて、期間中は
1. データのかさまし
2. 公開時のためのデータ構造の検討
3. プロンプトハッキングの調査
4. 翻訳の正当性の確保
5. 文章データの翻訳
6. 見せ方の検討
7. サイトの公開作業

の作業を行う予定であった。
期間中に各作業の結果とその作業の詳細について報告をします。

# 結果
1. 進歩はあり、部分的な課題解決は行えたが新たな課題に直面し、現在も解決に取り組んでいる。
データのかさましについては期間中、毎日3-6時間取り組み、作業としては進んだが進歩よりも新しく見つかる課題の方が大きかった。

2. まず初期ごろ、考えた理想に近い構造のデータ構造を作成した。だが当初想定していたものとは違う不安を抱えている。
データ構造については、当初は軽く思っていたが、入力データの多様さに直面し、初める前よりもわからなくなっている。自身の知見の不足を感じている。

3. プロンプトハッキングについては、幾つかの入力例の作成を自動で作るスクリプトを作成したが、精度の確認が未実施。

4. ローカルLLMを用いて小規模な実験を行ったが、評価指標の策定の途中で終わっている。

5. ローカルLLMではなくgpt-3とGeminiでの実験に留まっている。

6. ほとんど取り組んではいない。
7. ほとんど取り組んではいない。

## 全体の結果の要約としては
結果の成果物として、大規模に動かすための装置の開発と少量の翻訳済みデータ、未完成なウェブサイトがある状況。
LLMの調整ではなく、ほとんど文章管理システムの開発、古い入力データをちゃんとした形式に直すアダプターの開発をしていた。
精度向上,ハレーション検知システムの実装はできてないです。

作業の詳細について

1. データのかさまし
プロジェクトグーテンブルクと青空文庫の作品をそれぞれ(5、3ずつ)作品選び、それに対応できるスクレイピングの作成、それを適切に可搬性が高いデータ構造の作成を行った。
それを拡大させていく形で,他の作品を自動で収集する予定であった。
だが、なかなか転用に成功できることがなく、動かないことが多く原因の探求に時間を使うことになった。(地獄み)
結果として、グーテンブルクについては、作品によって入力のフォーマットがバラバラで、機械学習を用いたAIスクレイピングの作成から初める必要があることに気づいた。
プロジェクトグーテンブルクについては、HTMLの仕様が固まる以前の1970年代からデータの入力がされており、それを現代で転用するのは……地獄。(なんなら、本をOCRした方が早い)
だが、その課題の解決はグーテンブルク自体に対して貢献することにも繋がるので、個人的には興味があったのでリソースを割いてしまった。
AIスクレイピングの精度の高い実装を試み、それに時間をつかってしまった。
(cf. standaredebooks github)

2. 公開時のためのデータ構造の検討
まったく構造化されていない、文章データをルールベースで適切なフォーマットに変換し、一定のルールに基づいたデータ構造に修正することを試みていた。
データ構造については、かなり細かく、使いやすく作る、をモットーに作り込んだのだが…………だけど、世界の複雑さに勝てない。
最終的に、Latexと同じ水準でカバーできれば、問題ないだろう、ということで作業に区切りをつけた。

スタンダートエーブックス(standaredebooks)というプロジェクトがある。
プロジェクトグーテンバーグのデータを入力しなおして、機械判読可能なフォーマットにするプロジェクトにである。
ただし、人間が手動でやっていて、本の数はグーテンブルグの一万分の1くらいの量しかできてない。
またstandaredebooks はxmlでデータ構造にもつことを模範回答としているが……個人的には精度が低い。
翻訳用に、xmlをさらに細かなデータ構造に直す必要があるのであんまり好きじゃない。
Latexしかり、RFCで定義されるような業界標準を作るのであれば、古の知見も集めないといけないし、誰よりも専門を深くもたないといけない。
調べるの面倒……やる必要はないが、多分、やった方が早い。

3. 文脈をプロンプトに取り入れるための実装を複数試した。
実装した翻訳戦略の例
```
a. 翻訳に必要な情報を選抜して、それを元に要約情報をLLM自体に作らせて、その上で、翻訳をさせる。(トークン上限に応じて、文脈量、翻訳対象の量を調整する)
b. できる限り一気に翻訳する、でもいいのだけれども、どの範囲まで(どれくらいの割合のトークンを)文脈情報を入れるのが、よい翻訳に結びつくのか、の実験スクリプト
c. 入力の量に応じて、ハレーションの性質がどう変化するのか、のイテレーション実験を回すための実験環境の構築(評価基準をまだ迷っていて、動かせてない)
d. 章や段落、セクションの構造を活用した要約情報の作成。情報量の選別をしてプロンプトを作る戦略。
```

4. ハレーション検知について
ハレーション検知については、単語ベクトルの異常検知を実装する予定だったけど……できてない。悲しみ。

5. 文章データの翻訳
システム構築に時間を使いすぎてあんまり、精度の調整の水準まで至ってない……Colaboratoryで小さなモデルが動くか、動かないか、gemini,gpt-3との目視m比較までで終わってる。

6. サイト構築について
- GPU使える期間中にサイト構築するモチベーションない……


# 得たもの
やればやるほど、自分の無知に向き合うことになった。
GPU使ってる時間があんまりなかった……
個人的には、マイナー言語や論理学の文章の翻訳に関心があり、Sakanaのやっていた自然言語と数理AIのモデルマージを活用することに関心がある。
完成物作らないと、って焦っていたので試せてないが、今から一週間GPU使っていいよ、って言われたらSakanaの再現実装をすると思う。

# 裏テーマと後悔
文脈情報が及ぼす翻訳結果の違い、の実験に新規性があり、そこに到達したかった。ですが、振り返るとシステムが作りたいのか、実験がしたいのか曖昧にしてしまった。
GPUを使える、という機会でもあったので、実験にリソースをもっと振ってもよかったのではないか、という後悔がある。

# 感謝
このような機会を提供頂き、saldraさんを始め運営の方々、スポンサーの皆様、ありがとうございました。
